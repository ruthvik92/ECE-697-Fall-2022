{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "* [YT](https://www.youtube.com/watch?v=KRgq4VnCr7I&t=114s)\n",
    "* [YT-code](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/torchtext/torchtext_tutorial1.py)\n",
    "* [BucketIterator](https://gmihaila.medium.com/better-batches-with-pytorchtext-bucketiterator-12804a545e2a)\n",
    "* [BucketIterator](https://github.com/gmihaila/ml_things/blob/master/notebooks/pytorch/pytorchtext_bucketiterator.ipynb)\n",
    "* [PyTorch-Official-Tutorial](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)\n",
    "* [Variable sentence length](https://medium.com/@sonicboom8/sentiment-analysis-with-variable-length-sequences-in-pytorch-6241635ae130)\n",
    "* [torchtext.data.Field](https://pytorch.org/text/_modules/torchtext/data/field.html)\n",
    "* [torchtext.data.TabularDataset](https://pytorch.org/text/_modules/torchtext/data/dataset.html)\n",
    "* [tochtext.datasets.translation](https://pytorch.org/text/_modules/torchtext/datasets/translation.html)\n",
    "* [torchtext.data.BucketIterator](https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator)\n",
    "* [Sentiment-analysis-transformer](https://towardsdatascience.com/fine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6)\n",
    "* [Kaggle-IMDB dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy, io, os, sys\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "from IPython.display import display, HTML\n",
    "from torchtext.utils import unicode_csv_reader\n",
    "import transformer.Constants as Constants\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some environment checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torchtext version:0.8.0, Torch version:1.7.1\n",
      "Is CUDA available:True\n",
      "Device iscuda:0\n"
     ]
    }
   ],
   "source": [
    "print('Torchtext version:{}, Torch version:{}'.format(torchtext.__version__, torch.__version__))\n",
    "print('Is CUDA available:{}'.format(torch.cuda.is_available()))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device is{}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the tokenization for the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 3\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pre-built tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang_model = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(src_lang_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_meta': {'lang': 'en',\n",
       "  'name': 'core_web_sm',\n",
       "  'license': 'MIT',\n",
       "  'author': 'Explosion',\n",
       "  'url': 'https://explosion.ai',\n",
       "  'email': 'contact@explosion.ai',\n",
       "  'description': 'English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities.',\n",
       "  'sources': [{'name': 'OntoNotes 5',\n",
       "    'url': 'https://catalog.ldc.upenn.edu/LDC2013T19',\n",
       "    'license': 'commercial (licensed by Explosion)'}],\n",
       "  'pipeline': ['tagger', 'parser', 'ner'],\n",
       "  'version': '2.3.1',\n",
       "  'spacy_version': '>=2.3.0,<2.4.0',\n",
       "  'parent_package': 'spacy',\n",
       "  'accuracy': {'las': 89.7572754092,\n",
       "   'uas': 91.6570115569,\n",
       "   'token_acc': 99.756964111,\n",
       "   'las_per_type': {'advmod': {'p': 85.6065101297,\n",
       "     'r': 84.9512113055,\n",
       "     'f': 85.2776018577},\n",
       "    'aux': {'p': 97.9464841319, 'r': 98.0772654442, 'f': 98.0118311613},\n",
       "    'nsubj': {'p': 95.530627567, 'r': 94.7522887555, 'f': 95.1398662913},\n",
       "    'root': {'p': 89.5162856958, 'r': 91.1692936754, 'f': 90.3352283866},\n",
       "    'compound': {'p': 90.4871122761, 'r': 92.2811316552, 'f': 91.3753170839},\n",
       "    'poss': {'p': 97.0346623923, 'r': 97.4838969404, 'f': 97.2587609198},\n",
       "    'case': {'p': 97.927972373, 'r': 99.3493493493, 'f': 98.6335403727},\n",
       "    'dobj': {'p': 92.4513496547, 'r': 93.8729981675, 'f': 93.1567503459},\n",
       "    'prep': {'p': 85.6642170718, 'r': 86.2427438631, 'f': 85.9525069954},\n",
       "    'pobj': {'p': 96.0694769711, 'r': 96.6428459243, 'f': 96.3553084873},\n",
       "    'relcl': {'p': 76.5768958186, 'r': 78.3538796229, 'f': 77.4551971326},\n",
       "    'det': {'p': 97.7105145232, 'r': 97.7901904024, 'f': 97.7503362269},\n",
       "    'amod': {'p': 91.5748754262, 'r': 90.4891480402, 'f': 91.0287743996},\n",
       "    'attr': {'p': 90.4294478528, 'r': 92.9772918419, 'f': 91.6856728177},\n",
       "    'cc': {'p': 83.8244137102, 'r': 83.3532647692, 'f': 83.5881753313},\n",
       "    'mark': {'p': 90.3421052632, 'r': 90.9644939057, 'f': 90.6522313177},\n",
       "    'nmod': {'p': 76.3772954925, 'r': 55.7586837294, 'f': 64.4593166608},\n",
       "    'conj': {'p': 76.8877867328, 'r': 78.0490874764, 'f': 77.4640849469},\n",
       "    'advcl': {'p': 68.9203354298, 'r': 66.2301687232, 'f': 67.548478233},\n",
       "    'pcomp': {'p': 85.2515506547, 'r': 86.6246498599, 'f': 85.9326154915},\n",
       "    'nummod': {'p': 93.1951089846, 'r': 88.5353535354, 'f': 90.8054908055},\n",
       "    'nsubjpass': {'p': 92.4265842349, 'r': 92.0, 'f': 92.2127987664},\n",
       "    'quantmod': {'p': 85.3463587922, 'r': 78.0666125102, 'f': 81.5443360204},\n",
       "    'auxpass': {'p': 94.7204968944, 'r': 97.2665148064, 'f': 95.9766239604},\n",
       "    'ccomp': {'p': 79.9260844194, 'r': 83.6863543788, 'f': 81.7630086559},\n",
       "    'npadvmod': {'p': 76.8636539204, 'r': 70.6927175844, 'f': 73.6491487787},\n",
       "    'appos': {'p': 70.0960219479, 'r': 66.5075921909, 'f': 68.2546749777},\n",
       "    'neg': {'p': 94.5082376435, 'r': 94.9824385349, 'f': 94.7447447447},\n",
       "    'xcomp': {'p': 88.2854100106, 'r': 89.2677674085, 'f': 88.7738711405},\n",
       "    'predet': {'p': 85.2459016393, 'r': 89.2703862661, 'f': 87.2117400419},\n",
       "    'acomp': {'p': 90.3553299492, 'r': 88.717716357, 'f': 89.529035208},\n",
       "    'acl': {'p': 75.6578947368, 'r': 69.012547736, 'f': 72.182596291},\n",
       "    'oprd': {'p': 81.0810810811, 'r': 71.6417910448, 'f': 76.0697305864},\n",
       "    'dative': {'p': 73.9659367397, 'r': 69.7247706422, 'f': 71.7827626919},\n",
       "    'agent': {'p': 88.5328836425, 'r': 94.0860215054, 'f': 91.2250217202},\n",
       "    'meta': {'p': 94.7368421053, 'r': 34.615384615400004, 'f': 50.7042253521},\n",
       "    'dep': {'p': 40.329218107, 'r': 15.9090909091, 'f': 22.8172293364},\n",
       "    'prt': {'p': 81.9166666667, 'r': 88.082437276, 'f': 84.8877374784},\n",
       "    'expl': {'p': 98.3014861996, 'r': 99.1434689507, 'f': 98.7206823028},\n",
       "    'parataxis': {'p': 63.9240506329, 'r': 43.8177874187, 'f': 51.9948519949},\n",
       "    'intj': {'p': 69.387755102, 'r': 59.7802197802, 'f': 64.2266824085},\n",
       "    'csubj': {'p': 70.5882352941, 'r': 71.0059171598, 'f': 70.796460177},\n",
       "    'preconj': {'p': 57.4468085106, 'r': 62.7906976744, 'f': 60.0},\n",
       "    'csubjpass': {'p': 44.4444444444, 'r': 66.6666666667, 'f': 53.3333333333}},\n",
       "   'tags_acc': 97.056555292,\n",
       "   'ents_f': 85.4306864065,\n",
       "   'ents_p': 85.7239322492,\n",
       "   'ents_r': 85.1394400045,\n",
       "   'ents_per_type': {'ORG': {'p': 83.3194096352,\n",
       "     'r': 82.8808864266,\n",
       "     'f': 83.0995695042},\n",
       "    'CARDINAL': {'p': 83.9554682384, 'r': 86.32996633, 'f': 85.1261620186},\n",
       "    'DATE': {'p': 84.6522781775, 'r': 86.1275705821, 'f': 85.3835521769},\n",
       "    'GPE': {'p': 92.5831202046, 'r': 90.2180685358, 'f': 91.3852950458},\n",
       "    'PERSON': {'p': 88.0239520958, 'r': 92.0908379013, 'f': 90.0114810563},\n",
       "    'MONEY': {'p': 92.9181929182, 'r': 91.4663461538, 'f': 92.1865536039},\n",
       "    'PRODUCT': {'p': 52.1276595745, 'r': 24.1379310345, 'f': 32.9966329966},\n",
       "    'TIME': {'p': 70.1886792453, 'r': 70.9923664122, 'f': 70.5882352941},\n",
       "    'PERCENT': {'p': 91.8566775244, 'r': 88.125, 'f': 89.95215311},\n",
       "    'WORK_OF_ART': {'p': 48.1481481481,\n",
       "     'r': 38.8059701493,\n",
       "     'f': 42.9752066116},\n",
       "    'QUANTITY': {'p': 78.1954887218, 'r': 65.4088050314, 'f': 71.2328767123},\n",
       "    'NORP': {'p': 88.682581786, 'r': 89.2348754448, 'f': 88.9578713969},\n",
       "    'LOC': {'p': 70.8154506438, 'r': 66.0, 'f': 68.3229813665},\n",
       "    'EVENT': {'p': 63.2911392405, 'r': 37.037037037, 'f': 46.7289719626},\n",
       "    'ORDINAL': {'p': 80.0, 'r': 83.9694656489, 'f': 81.9366852886},\n",
       "    'FAC': {'p': 34.8623853211, 'r': 46.9135802469, 'f': 40.0},\n",
       "    'LAW': {'p': 62.962962963, 'r': 56.6666666667, 'f': 59.649122807},\n",
       "    'LANGUAGE': {'p': 75.0, 'r': 65.2173913043, 'f': 69.7674418605}}},\n",
       "  'speed': {'cpu': 6107.3535050376, 'gpu': None, 'nwords': 291315},\n",
       "  'labels': OrderedDict([('tagger',\n",
       "                ['\"\"',\n",
       "                 '#',\n",
       "                 '$',\n",
       "                 \"''\",\n",
       "                 ',',\n",
       "                 '-LRB-',\n",
       "                 '-RRB-',\n",
       "                 '.',\n",
       "                 ':',\n",
       "                 'ADD',\n",
       "                 'AFX',\n",
       "                 'BES',\n",
       "                 'CC',\n",
       "                 'CD',\n",
       "                 'DT',\n",
       "                 'EX',\n",
       "                 'FW',\n",
       "                 'GW',\n",
       "                 'HVS',\n",
       "                 'HYPH',\n",
       "                 'IN',\n",
       "                 'JJ',\n",
       "                 'JJR',\n",
       "                 'JJS',\n",
       "                 'LS',\n",
       "                 'MD',\n",
       "                 'NFP',\n",
       "                 'NIL',\n",
       "                 'NN',\n",
       "                 'NNP',\n",
       "                 'NNPS',\n",
       "                 'NNS',\n",
       "                 'PDT',\n",
       "                 'POS',\n",
       "                 'PRP',\n",
       "                 'PRP$',\n",
       "                 'RB',\n",
       "                 'RBR',\n",
       "                 'RBS',\n",
       "                 'RP',\n",
       "                 'SP',\n",
       "                 'SYM',\n",
       "                 'TO',\n",
       "                 'UH',\n",
       "                 'VB',\n",
       "                 'VBD',\n",
       "                 'VBG',\n",
       "                 'VBN',\n",
       "                 'VBP',\n",
       "                 'VBZ',\n",
       "                 'WDT',\n",
       "                 'WP',\n",
       "                 'WP$',\n",
       "                 'WRB',\n",
       "                 'XX',\n",
       "                 '_SP',\n",
       "                 '``']),\n",
       "               ('parser', []),\n",
       "               ('ner', [])]),\n",
       "  'spacy_git_version': '1d4b1dea2',\n",
       "  'vectors': {'width': 0, 'vectors': 0, 'keys': 0, 'name': None},\n",
       "  'factories': {'tagger': 'tagger', 'parser': 'parser', 'ner': 'ner'}},\n",
       " '_path': PosixPath('/home/visionteam/python37_env/lib/python3.7/site-packages/spacy/data/en/en_core_web_sm-2.3.1'),\n",
       " 'vocab': <spacy.vocab.Vocab at 0x7fbbc780ae60>,\n",
       " 'tokenizer': <spacy.tokenizer.Tokenizer at 0x7fbbc7808200>,\n",
       " 'pipeline': [('tagger', <spacy.pipeline.pipes.Tagger at 0x7fbbc7812410>),\n",
       "  ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7fbbc77ef590>),\n",
       "  ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7fbbc77ef9f0>)],\n",
       " 'max_length': 1000000,\n",
       " '_optimizer': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(src_lang_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I am a good person, and I have a few dollars $!! and no dollars"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip_text = 'I am a good person, and I have a few dollars $!! and no dollars'\n",
    "temp = src_lang_model.tokenizer(ip_text)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_src(ip_text):\n",
    "    return [tok.text for tok in src_lang_model.tokenizer(ip_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `source`, and `target` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'good', 'person', ',', 'and', 'I', 'have', 'a', 'few', 'dollars', '$', '!', '!', 'and', 'no', 'dollars']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_src('I am a good person, and I have a few dollars $!! and no dollars'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Field(tokenize=tokenize_src, lower=True, pad_token=Constants.PAD_WORD, sequential=True,\n",
    "               init_token=Constants.BOS_WORD, eos_token=Constants.EOS_WORD, is_target=False)\n",
    "label = Field(sequential=False, use_vocab=False, is_target=True)\n",
    "fields = {'text':('text', text), 'label':('label', label)} ## a dictionary of tuples.\n",
    "#fields = [('text', text), ('label', label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([('text', <torchtext.data.field.Field object at 0x7fbbabdb7910>), ('label', <torchtext.data.field.Field object at 0x7fbbabdb7d50>)])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "````##Source: https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string\n",
    "path = '/home/visionteam/tf_tutorials/imdb_dataset/Test.csv'\n",
    "with io.open(os.path.expanduser(path), encoding=\"utf-8\") as f:\n",
    "    reader = unicode_csv_reader(f, delimiter=',')\n",
    "    print(next(reader))\n",
    "    ['\\ufefftext', 'label'] <-- Notice the extra \\ufeff, to prevent this use encoding=\"utf-8-sig\"\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'imdb_dataset/Train.csv'\n",
    "df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "display(df.head())\n",
    "df.to_csv(path, encoding='utf-8', index=False)\n",
    "df = pd.read_csv(path)\n",
    "display(df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'imdb_dataset/Test.csv'\n",
    "df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "display(df.head())\n",
    "df.to_csv(path, encoding='utf-8', index=False)\n",
    "df = pd.read_csv(path)\n",
    "display(df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'imdb_dataset/Valid.csv'\n",
    "df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "display(df.head())\n",
    "df.to_csv(path, encoding='utf-8', index=False)\n",
    "df = pd.read_csv(path)\n",
    "display(df.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* x is an object that contains 'text', and 'label' attributes. Value of the 'text' is simply the input sentence, and the value of the 'label' is simply the label for the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_examples_with_length(x):\n",
    "    return len(vars(x)['text']) <= MAX_LEN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/visionteam/python37_env/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/visionteam/python37_env/lib/python3.7/site-packages/torchtext/data/example.py:52: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, valid_data = TabularDataset.splits(path='/home/visionteam/tf_tutorials/imdb_dataset', \n",
    "                                                          train='Train.csv',validation='Valid.csv', \n",
    "                                                          test='Test.csv',format='csv', fields=fields,\n",
    "                                                         filter_pred=filter_examples_with_length)\n",
    "# <--- instantiates a Dataset class and return it\n",
    "# try without passing filter_pred=filter_examples_with_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['someone',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'car',\n",
       " 'payment',\n",
       " '...',\n",
       " 'this',\n",
       " 'is',\n",
       " 'truly',\n",
       " 'awful',\n",
       " '...',\n",
       " 'makes',\n",
       " 'jean',\n",
       " 'claude',\n",
       " \"'s\",\n",
       " 'cyborg',\n",
       " 'look',\n",
       " 'like',\n",
       " 'gone',\n",
       " 'with',\n",
       " 'the',\n",
       " 'wind',\n",
       " '...',\n",
       " 'this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'hour',\n",
       " 'i',\n",
       " 'wish',\n",
       " 'i',\n",
       " 'could',\n",
       " 'sue',\n",
       " 'to',\n",
       " 'get',\n",
       " 'back',\n",
       " '...',\n",
       " 'luckily',\n",
       " 'it',\n",
       " 'produced',\n",
       " 'severe',\n",
       " 'somnolence',\n",
       " '...',\n",
       " 'from',\n",
       " 'which',\n",
       " 'i',\n",
       " 'fell',\n",
       " 'asleep',\n",
       " '.',\n",
       " 'how',\n",
       " 'can',\n",
       " 'actors',\n",
       " 'of',\n",
       " 'this',\n",
       " 'caliber',\n",
       " 'create',\n",
       " 'this',\n",
       " 'dog',\n",
       " '?',\n",
       " 'i',\n",
       " 'would',\n",
       " 'rather',\n",
       " 'spend',\n",
       " 'the',\n",
       " 'time',\n",
       " 'watching',\n",
       " 'algae',\n",
       " 'grow',\n",
       " 'on',\n",
       " 'the',\n",
       " 'side',\n",
       " 'of',\n",
       " 'a',\n",
       " 'fish',\n",
       " 'tank',\n",
       " 'than',\n",
       " 'partake',\n",
       " 'of',\n",
       " 'this',\n",
       " 'wholly',\n",
       " 'awful',\n",
       " 'concoction',\n",
       " 'of',\n",
       " 'several',\n",
       " 'genre',\n",
       " '.',\n",
       " 'i',\n",
       " 'now',\n",
       " 'use',\n",
       " 'the',\n",
       " 'dvd',\n",
       " 'as',\n",
       " 'a',\n",
       " 'coaster',\n",
       " 'on',\n",
       " 'my',\n",
       " 'coffee',\n",
       " 'table',\n",
       " '.',\n",
       " '$',\n",
       " '5.99',\n",
       " 'at',\n",
       " 'walmart',\n",
       " 'is',\n",
       " 'far',\n",
       " 'too',\n",
       " 'much',\n",
       " 'to',\n",
       " 'spend',\n",
       " 'on',\n",
       " 'this',\n",
       " 'movie',\n",
       " '...',\n",
       " 'if',\n",
       " 'you',\n",
       " 'really',\n",
       " 'have',\n",
       " 'to',\n",
       " 'have',\n",
       " 'it',\n",
       " ',',\n",
       " 'wait',\n",
       " 'till',\n",
       " 'they',\n",
       " 'throw',\n",
       " 'them',\n",
       " 'out',\n",
       " 'after',\n",
       " 'they',\n",
       " 'have',\n",
       " 'carried',\n",
       " 'them',\n",
       " 'on',\n",
       " 'the',\n",
       " 'inventory',\n",
       " 'for',\n",
       " 'several',\n",
       " 'years',\n",
       " 'and',\n",
       " 'are',\n",
       " 'frustrated',\n",
       " 'that',\n",
       " 'they',\n",
       " 'would',\n",
       " 'not',\n",
       " 'sell.<br',\n",
       " '/><br',\n",
       " '/>please',\n",
       " 'for',\n",
       " 'the',\n",
       " 'love',\n",
       " 'of',\n",
       " 'god',\n",
       " 'let',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'die',\n",
       " 'of',\n",
       " 'obscurity',\n",
       " '.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(x)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'label'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = test_data[0]\n",
    "vars(x).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3268"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25588"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.data.dataset.TabularDataset"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['examples', 'fields'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example text:['i', 'grew', 'up', '(', 'b.', '1965', ')', 'watching', 'and', 'loving', 'the', 'thunderbirds', '.', 'all', 'my', 'mates', 'at', 'school', 'watched', '.', 'we', 'played', '\"', 'thunderbirds', '\"', 'before', 'school', ',', 'during', 'lunch', 'and', 'after', 'school', '.', 'we', 'all', 'wanted', 'to', 'be', 'virgil', 'or', 'scott', '.', 'no', 'one', 'wanted', 'to', 'be', 'alan', '.', 'counting', 'down', 'from', '5', 'became', 'an', 'art', 'form', '.', 'i', 'took', 'my', 'children', 'to', 'see', 'the', 'movie', 'hoping', 'they', 'would', 'get', 'a', 'glimpse', 'of', 'what', 'i', 'loved', 'as', 'a', 'child', '.', 'how', 'bitterly', 'disappointing', '.', 'the', 'only', 'high', 'point', 'was', 'the', 'snappy', 'theme', 'tune', '.', 'not', 'that', 'it', 'could', 'compare', 'with', 'the', 'original', 'score', 'of', 'the', 'thunderbirds', '.', 'thankfully', 'early', 'saturday', 'mornings', 'one', 'television', 'channel', 'still', 'plays', 'reruns', 'of', 'the', 'series', 'gerry', 'anderson', 'and', 'his', 'wife', 'created', '.', 'jonatha', 'frakes', 'should', 'hand', 'in', 'his', 'directors', 'chair', ',', 'his', 'version', 'was', 'completely', 'hopeless', '.', 'a', 'waste', 'of', 'film', '.', 'utter', 'rubbish', '.', 'a', 'cgi', 'remake', 'may', 'be', 'acceptable', 'but', 'replacing', 'marionettes', 'with', 'homo', 'sapiens', 'subsp', '.', 'sapiens', 'was', 'a', 'huge', 'error', 'of', 'judgment', '.']\n",
      "\n",
      "Example label:0\n"
     ]
    }
   ],
   "source": [
    "example_text = vars(train_data.examples[0])['text']\n",
    "print('Example text:{}\\n'.format(example_text))\n",
    "example_label = vars(train_data.examples[0])['label']\n",
    "print('Example label:{}'.format(example_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': <torchtext.data.field.Field at 0x7fbbabdb7910>,\n",
       " 'label': <torchtext.data.field.Field at 0x7fbbabdb7d50>}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "print(list(text.vocab.stoi.values())[0:20]) ## shouldn't work because we haven't build the vocabulary yet, and try making\n",
    "# the BucketIterator without building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<blank>', '<s>', '</s>', 'the', '.', ',', 'and', 'a', 'of', 'to', 'is', 'i', 'it', 'this', 'in', 'that', 'movie', 'was', '\"']\n"
     ]
    }
   ],
   "source": [
    "print(list(text.vocab.stoi.keys())[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30195\n"
     ]
    }
   ],
   "source": [
    "print(len(list(text.vocab.stoi.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.build_vocab(train_data.text, min_freq=MIN_FREQ)\n",
    "#text.build_vocab(valid_data.text, min_freq=MIN_FREQ)\n",
    "#text.build_vocab(test_data.text, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Field (that has built vocabulary), and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'fields_with_and_without_vocab': {'text': text, 'label':label},\n",
    "    'train_examples': train_data.examples,\n",
    "    'valid_examples': valid_data.examples,\n",
    "    'test_examples': test_data.examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/visionteam/tf_tutorials/imdb_dataset/imdb_fields_and_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "save_data = os.path.expanduser('~/tf_tutorials/imdb_dataset/imdb_fields_and_vocab.pkl')\n",
    "print(save_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data, open(save_data, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(save_data, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<blank>', '<s>', '</s>', 'the', '.', ',', 'and', 'a', 'of', 'to', 'is', 'i', 'it', 'this', 'in', 'that', 'movie', 'was', '\"']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Â·\n",
      "30194\n"
     ]
    }
   ],
   "source": [
    "print(list(data['fields_with_and_without_vocab']['text'].vocab.stoi.keys())[0:20])\n",
    "print(list(data['fields_with_and_without_vocab']['text'].vocab.stoi.values())[0:20])\n",
    "print\n",
    "print(list(data['fields_with_and_without_vocab']['text'].vocab.stoi.keys())[-1])\n",
    "print(list(data['fields_with_and_without_vocab']['text'].vocab.stoi.values())[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/visionteam/python37_env/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iterator, test_iterator, valid_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data, valid_data), batch_size=2, device=device\n",
    ")  #<--- bucketiterator expects a dataset object and fields that already have vocabularay built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12794"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1634"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1611"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/visionteam/python37_env/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([225, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([230, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([203, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([174, 2])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/visionteam/python37_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3425: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for batch in train_iterator:\n",
    "    print(batch.text.shape)\n",
    "    print(batch.label.shape)\n",
    "    count += 1\n",
    "    if(count == 5):\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     2],\n",
       "        [   12,    12],\n",
       "        [  877,    86],\n",
       "        [   14,    29],\n",
       "        [  490,   705],\n",
       "        [   31,    10],\n",
       "        [   46,  4375],\n",
       "        [11111,     4],\n",
       "        [  578,   237],\n",
       "        [   51,   127],\n",
       "        [10849,  2502],\n",
       "        [    5,    42],\n",
       "        [   12,    91],\n",
       "        [  202,    54],\n",
       "        [  927,     4],\n",
       "        [   12,    26],\n",
       "        [   81,     5],\n",
       "        [    5,    60],\n",
       "        [   13,     8],\n",
       "        [   18,  1192],\n",
       "        [   59,    33],\n",
       "        [  379,    14],\n",
       "        [ 2406,   896],\n",
       "        [   10,    35],\n",
       "        [ 1694,     9],\n",
       "        [ 2918,     4],\n",
       "        [    7,   109],\n",
       "        [  454,  1380],\n",
       "        [    4,   131],\n",
       "        [  490,    12],\n",
       "        [    5,   146],\n",
       "        [   12,   112],\n",
       "        [  177,  1742],\n",
       "        [    4,     5],\n",
       "        [ 2488,    56],\n",
       "        [   82,    82],\n",
       "        [    4,    42],\n",
       "        [  125,   129],\n",
       "        [   12,   122],\n",
       "        [   32,  2328],\n",
       "        [  112,     6],\n",
       "        [  102,    62],\n",
       "        [   15,  6974],\n",
       "        [    8,   434],\n",
       "        [   17,     6],\n",
       "        [  158,    16],\n",
       "        [  490,    13],\n",
       "        [  960,   896],\n",
       "        [    5,  1174],\n",
       "        [    4,    10],\n",
       "        [ 3818,   829],\n",
       "        [ 2061,     4],\n",
       "        [   82,    71],\n",
       "        [   59,   351],\n",
       "        [  258,     5],\n",
       "        [   33,    12],\n",
       "        [    4,    86],\n",
       "        [ 6490,    30],\n",
       "        [   35,   369],\n",
       "        [   12,   100],\n",
       "        [  108,   795],\n",
       "        [   18,  5659],\n",
       "        [  113,  1930],\n",
       "        [  258,    46],\n",
       "        [    6,   101],\n",
       "        [   13,  1267],\n",
       "        [   99,    23],\n",
       "        [   73,     4],\n",
       "        [   42,    26],\n",
       "        [ 3599,   266],\n",
       "        [   12,    44],\n",
       "        [   81,    18],\n",
       "        [   29,  1044],\n",
       "        [  331,   574],\n",
       "        [    4,     6],\n",
       "        [  490,    40],\n",
       "        [   23,   242],\n",
       "        [    8,    15],\n",
       "        [ 1081,     4],\n",
       "        [    7,   107],\n",
       "        [    8,   344],\n",
       "        [  344,     9],\n",
       "        [    5,     4],\n",
       "        [   56,    17],\n",
       "        [   82,     5],\n",
       "        [  113,    61],\n",
       "        [  129,   410],\n",
       "        [   93,     7],\n",
       "        [ 1317,    12],\n",
       "        [   40,   439],\n",
       "        [   25,    77],\n",
       "        [   28,   127],\n",
       "        [  171,    54],\n",
       "        [   10,    46],\n",
       "        [ 2503,   509],\n",
       "        [  634,     9],\n",
       "        [   57,  5055],\n",
       "        [  134,     7],\n",
       "        [   10,  1885],\n",
       "        [ 2990,     4],\n",
       "        [   13,   237],\n",
       "        [    5,     5],\n",
       "        [  124,    12],\n",
       "        [  126,   271],\n",
       "        [   21,    32],\n",
       "        [  137,  1254],\n",
       "        [   78,   638],\n",
       "        [  159,  7084],\n",
       "        [   12,    10],\n",
       "        [  271,    65],\n",
       "        [  728,    52],\n",
       "        [   18,     4],\n",
       "        [    4,   275],\n",
       "        [19506,    18],\n",
       "        [ 1563,   120],\n",
       "        [    5,   128],\n",
       "        [   12,    24],\n",
       "        [   81,   127],\n",
       "        [   29,    42],\n",
       "        [ 1423,    91],\n",
       "        [  120,  3592],\n",
       "        [ 1616,   839],\n",
       "        [   51,    16],\n",
       "        [ 2551,  3821],\n",
       "        [   22,    23],\n",
       "        [29896,  4752],\n",
       "        [ 5104,    82],\n",
       "        [   10,  6650],\n",
       "        [    4,     5],\n",
       "        [29896,     8],\n",
       "        [    6,  1320],\n",
       "        [  162,    22],\n",
       "        [   27,  4852],\n",
       "        [ 2006,   633],\n",
       "        [   46,    73],\n",
       "        [12003,    16],\n",
       "        [19506,     8],\n",
       "        [   81,   173],\n",
       "        [   29,     9],\n",
       "        [  366,     4],\n",
       "        [   10,    17],\n",
       "        [  105,    19],\n",
       "        [    4,   350],\n",
       "        [12003,   318],\n",
       "        [ 4390,    19],\n",
       "        [   57,    15],\n",
       "        [   66,     4],\n",
       "        [ 5483,   265],\n",
       "        [   31,   837],\n",
       "        [   61,    57],\n",
       "        [  134,   416],\n",
       "        [10204,     5],\n",
       "        [    5,    12],\n",
       "        [   12,    18],\n",
       "        [   67,   927],\n",
       "        [  348,    12],\n",
       "        [    4,    81],\n",
       "        [ 6188,    29],\n",
       "        [    0,   304],\n",
       "        [    5,   198],\n",
       "        [   12,   509],\n",
       "        [  108,     6],\n",
       "        [    4,   962],\n",
       "        [22897,     5],\n",
       "        [   81,    12],\n",
       "        [    8,   439],\n",
       "        [   53,     4],\n",
       "        [  308,   237],\n",
       "        [    5,   272],\n",
       "        [    3,     4],\n",
       "        [    1,  1210],\n",
       "        [    1,   296],\n",
       "        [    1,     5],\n",
       "        [    1,     3]], device='cuda:0')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37_env",
   "language": "python",
   "name": "python37_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "192.167px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
