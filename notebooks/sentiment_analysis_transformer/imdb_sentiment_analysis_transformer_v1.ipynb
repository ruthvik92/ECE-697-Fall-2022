{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-95jn9ozn because the default path (/home/visionteam/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import torch, sys, os\n",
    "import dill as pickle\n",
    "import torch.nn as nn\n",
    "import torch, torchtext \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import transformer.Constants as Constants\n",
    "from transformer.Layers import EncoderLayer, DecoderLayer\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some environment checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torchtext version:0.8.0, Torch version:1.7.1\n",
      "Is CUDA available:True\n",
      "Device iscuda:0\n"
     ]
    }
   ],
   "source": [
    "print('Torchtext version:{}, Torch version:{}'.format(torchtext.__version__, torch.__version__))\n",
    "print('Is CUDA available:{}'.format(torch.cuda.is_available()))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device is{}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/visionteam/tf_tutorials/imdb_dataset/imdb_fields_and_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "save_data = os.path.expanduser('~/tf_tutorials/imdb_dataset/imdb_fields_and_vocab.pkl')\n",
    "print(save_data)\n",
    "data = pickle.load(open(save_data, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fields_with_and_without_vocab', 'train_examples', 'valid_examples', 'test_examples'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = data['fields_with_and_without_vocab']\n",
    "train_data = Dataset(examples=data['train_examples'], fields=fields)\n",
    "valid_data = Dataset(examples=data['valid_examples'], fields=fields)\n",
    "test_data = Dataset(examples=data['test_examples'], fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30195"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fields['text'].vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['freqs', 'itos', 'unk_index', 'stoi', 'vectors'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(fields['text'].vocab).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': <torchtext.data.field.Field at 0x7f6db3e60d10>,\n",
       " 'label': <torchtext.data.field.Field at 0x7f6db373b810>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25588"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.examples[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': <torchtext.data.field.Field at 0x7f6db3e60d10>,\n",
       " 'label': <torchtext.data.field.Field at 0x7f6db373b810>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/visionteam/python37_env/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iterator, test_iterator, valid_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data, valid_data), batch_size=2, device=device\n",
    ")  #<--- bucketiterator expects a dataset object and fields that already have vocabularay built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/visionteam/python37_env/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([216, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([200, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([221, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([162, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([151, 2])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/visionteam/python37_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3425: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for batch in train_iterator:\n",
    "    print(batch.text.shape)\n",
    "    print(batch.label.shape)\n",
    "    count += 1\n",
    "    if(count == 5):\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "\n",
    "def get_subsequent_mask(seq):\n",
    "    ''' For masking out the subsequent info. '''\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = (1 - torch.triu(\n",
    "        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find the batch with smallest samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,   461,  6476,    40,   780,     6,    24,   266,   305,  6943,\n",
      "             5,  9759,  3787,    15,     4,   506,     9,  4144,  1526,     5,\n",
      "             8,   343,   429,     9, 11562,  1396,     5,    35,     9,    61,\n",
      "          1382,   131,     5,   155,    58,    29,     4,   636,    97,   854,\n",
      "            22,   575,    63,     3,     1,     1,     1,     1,     1],\n",
      "        [    2,    14,    11,     8,   698,     6,   135,     6,  1481,    17,\n",
      "            51,     4,   561,  3129,     5,    13,    20,    35,     9,   166,\n",
      "            19,  1233,    22,   180,  3876,  6334,    19,    89,    16,  1599,\n",
      "            62,    53,  1647,     5,    52,    25,    41,     4,    93,    15,\n",
      "            14,    35,     6,   199,    13,     8,   387,     5,     3]],\n",
      "       device='cuda:0')\n",
      "torch.Size([49, 2])\n"
     ]
    }
   ],
   "source": [
    "largest_seq_len = max([batch.text.shape[0] for batch in train_iterator])\n",
    "small_batch = None\n",
    "for batch in train_iterator:\n",
    "    if(batch.text.shape[0] < largest_seq_len):\n",
    "        small_batch = batch\n",
    "        largest_seq_len = batch.text.shape[0]\n",
    "print(small_batch.text.transpose(0,1))\n",
    "print(small_batch.text.shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find the batch with largest samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the largest sequence:258 and the smallest sequence:38\n"
     ]
    }
   ],
   "source": [
    "largest_seq_len = max([batch.text.shape[0] for batch in train_iterator])\n",
    "smallest_seq_len = min([batch.text.shape[0] for batch in train_iterator])\n",
    "print('Length of the largest sequence:{} and the smallest sequence:{}'.format(largest_seq_len,\n",
    "                                                                             smallest_seq_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test masking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequence shape:torch.Size([2, 49])\n"
     ]
    }
   ],
   "source": [
    "src_pad_idx = fields['text'].vocab.stoi[Constants.PAD_WORD]\n",
    "src_seq = small_batch.text.transpose(0,1)\n",
    "print('Source sequence shape:{}'.format(src_seq.shape))\n",
    "src_pad_mask = get_pad_mask(src_seq, src_pad_idx)\n",
    "src_subseq_mask = get_subsequent_mask(src_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 49])\n",
      "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True, False, False, False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           True,  True,  True,  True,  True,  True,  True,  True,  True]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(src_pad_mask.shape)\n",
    "print(src_pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49, 49])\n",
      "tensor([[[ True, False, False,  ..., False, False, False],\n",
      "         [ True,  True, False,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [ True,  True,  True,  ...,  True, False, False],\n",
      "         [ True,  True,  True,  ...,  True,  True, False],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(src_subseq_mask.shape)\n",
    "print(src_subseq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False,  ..., False, False, False],\n",
      "         [ True,  True, False,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False]],\n",
      "\n",
      "        [[ True, False, False,  ..., False, False, False],\n",
      "         [ True,  True, False,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [ True,  True,  True,  ...,  True, False, False],\n",
      "         [ True,  True,  True,  ...,  True,  True, False],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:0')\n",
      "torch.Size([2, 49, 49])\n"
     ]
    }
   ],
   "source": [
    "src_mask = src_pad_mask & src_subseq_mask\n",
    "print(src_mask)\n",
    "print(src_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False,  ..., False, False, False],\n",
      "        [ True,  True, False,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(src_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False,  ..., False, False, False],\n",
      "        [ True,  True, False,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ...,  True, False, False],\n",
      "        [ True,  True,  True,  ...,  True,  True, False],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(src_mask[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention\n",
    "![alt text](scaled_dot_product_attn.png \"Scaled Dot Product Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        #print(q.shape, k.shape, v.shape)\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
    "        #print(attn.shape)\n",
    "\n",
    "        if mask is not None:\n",
    "            #print('mask:{}'.format(mask.shape))\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        #print(attn.shape)\n",
    "        output = torch.matmul(attn, v)\n",
    "        #print(output.shape)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = 1\n",
    "batch = 2\n",
    "sent_len = small_batch.text.shape[0]\n",
    "d_model = 512\n",
    "d_k = d_model\n",
    "temperature = d_k ** 0.5\n",
    "q = torch.Tensor(np.random.rand(batch, sent_len, n_head, d_model)).to(device=device) ##fake sentence\n",
    "k = torch.Tensor(np.random.rand(batch, sent_len, n_head, d_model)).to(device=device)\n",
    "v = torch.Tensor(np.random.rand(batch, sent_len, n_head, d_model)).to(device=device)\n",
    "print('Query, Key, and Value shapes:{}, {}, {}'.format(q.shape, k.shape, v.shape))\n",
    "scaled_dpa = ScaledDotProductAttention(temperature=temperature)\n",
    "q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "print('Query, Key, and Value shapes:{}, {}, {}'.format(q.shape, k.shape, v.shape))\n",
    "src_mask = src_mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "output, attn = scaled_dpa(q, k, v, mask=src_mask)\n",
    "print('Output shape:{}, attention shape:{}'.format(output.shape, attn.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_attn = attn.squeeze(1).cpu().numpy()\n",
    "first_attention_mask = np.round(numpy_attn[0,:,:], 5)\n",
    "second_attention_mask = np.round(numpy_attn[1,:,:], 5)\n",
    "print(first_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(second_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention\n",
    "![alt text](multi-head-attention.png \"Multi-head attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "        # Separate different heads: b x lq x n x dv\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        # Transpose for attention dot product: b x n x lq x dv\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "        q, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Transpose to move the head dimension back: b x lq x n x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "        q = self.dropout(self.fc(q))\n",
    "        q += residual\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "\n",
    "        return q, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad_idx = fields['text'].vocab.stoi[Constants.PAD_WORD]\n",
    "src_seq = small_batch.text.transpose(0,1)\n",
    "print('Source sequence shape:{}'.format(src_seq.shape))\n",
    "src_pad_mask = get_pad_mask(src_seq, src_pad_idx)\n",
    "src_subseq_mask = get_subsequent_mask(src_seq)\n",
    "src_mask = src_pad_mask & src_subseq_mask\n",
    "print(src_mask)\n",
    "print(src_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = 8\n",
    "batch = 2\n",
    "sent_len = small_batch.text.shape[0]\n",
    "d_model = 512\n",
    "d_k = int(d_model / n_head)\n",
    "temperature = d_k ** 0.5\n",
    "q = torch.Tensor(np.random.rand(batch, sent_len, n_head*d_k)).to(device=device) ##fake sentence\n",
    "k = torch.Tensor(np.random.rand(batch, sent_len, n_head*d_k)).to(device=device)\n",
    "v = torch.Tensor(np.random.rand(batch, sent_len, n_head*d_k)).to(device=device)\n",
    "print('Shapes of Query:{}, Key:{}, and Value:{}'.format(q.shape, k.shape, v.shape))\n",
    "#q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "#print('Shapes of Query:{}, Key:{}, and Value:{}'.format(q.shape, k.shape, v.shape))\n",
    "mha = MultiHeadAttention(n_head=n_head, d_model=d_model, d_k=d_k, d_v=d_k)\n",
    "q, attn = mha(q=q, k=k, v=v, mask=src_mask) \n",
    "print('Shapes of q:{}, and attn:{}'.format(q.shape, attn.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attn[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attn[1,0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positionwise feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n",
    "        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder layer\n",
    "![alt text](encoder_without_positional.png \"Encoder without positional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    ''' Compose with two layers '''\n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, slf_attn_mask=None):\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output, enc_slf_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the EncoderLayer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad_idx = fields['text'].vocab.stoi[Constants.PAD_WORD]\n",
    "src_seq = small_batch.text.transpose(0,1)\n",
    "print('Source sequence shape:{}'.format(src_seq.shape))\n",
    "src_pad_mask = get_pad_mask(src_seq, src_pad_idx)\n",
    "src_subseq_mask = get_subsequent_mask(src_seq)\n",
    "src_mask = src_pad_mask & src_subseq_mask\n",
    "print(src_mask)\n",
    "print(src_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "d_inner = 1024\n",
    "n_head = 8\n",
    "d_k = int(d_model / n_head)\n",
    "d_v = d_k\n",
    "enc_layer = EncoderLayer(d_model=d_model, d_inner=d_inner, n_head=n_head, d_k=d_k, d_v=d_v)\n",
    "q = torch.Tensor(np.random.rand(batch, sent_len, n_head*d_k)).to(device=device) ##fake sentence\n",
    "print('Shapes of Query:{}, Key:{}, and Value:{}'.format(q.shape, k.shape, v.shape))\n",
    "enc_output, enc_self_attn = enc_layer(enc_input=q, slf_attn_mask=src_mask)\n",
    "print('enc_output.shape:{}, enc_self_attn.shape:{}'.format(enc_output.shape, enc_self_attn.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enc_self_attn[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enc_self_attn[1,0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "* Returns a tensor of size `n_position x d_model`, here `n_position` is the maximum number of words in your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_hid, n_position=300):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Not a parameter\n",
    "        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid))\n",
    "\n",
    "    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n",
    "        ''' Sinusoid position encoding table '''\n",
    "\n",
    "        def get_position_angle_vec(position):\n",
    "            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
    "\n",
    "        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_table[:, :x.size(1)].clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc = PositionalEncoding(100, n_position=20)\n",
    "x = np.zeros(shape=(20, 100))\n",
    "x = torch.Tensor(x)  ## <--- send in an empty tensor so that we can visualize what's inside\n",
    "x_pos_enc = pos_enc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos_enc = x_pos_enc.detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what's in even positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    plt.plot(range(20), x_pos_enc[:,2*i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what's in odd positions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    plt.plot(range(20), x_pos_enc[:,2*i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole Encoder\n",
    "![alt text](whole_encoder.png \"Whole encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    ''' A encoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, pad_idx, dropout=0.1, n_position=300, scale_emb=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx)\n",
    "        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.scale_emb = scale_emb\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src_seq, src_mask, return_attns=False):\n",
    "\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        # -- Forward\n",
    "        enc_output = self.src_word_emb(src_seq)\n",
    "        if self.scale_emb:\n",
    "            enc_output *= self.d_model ** 0.5\n",
    "        enc_output = self.dropout(self.position_enc(enc_output))\n",
    "        enc_output = self.layer_norm(enc_output)\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask)\n",
    "            enc_slf_attn_list += [enc_slf_attn] if return_attns else []\n",
    "\n",
    "        if return_attns:\n",
    "            return enc_output, enc_slf_attn_list\n",
    "        return enc_output,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the whole encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "d_k = int(d_model / n_head) #---> int() is very important or else you'll see\n",
    "TypeError: new() received an invalid combination of arguments - got (float, int), but expected one of:\n",
    " * (*, torch.device device)\n",
    "      didn't match because some of the arguments have invalid types: (float, int)\n",
    " * (torch.Storage storage)\n",
    " * (Tensor other)\n",
    " * (tuple of ints size, *, torch.device device)\n",
    " * (object data, *, torch.device device)\n",
    " ````\n",
    " * If you do not use `int()` then `d_k = 64.0`, and `d_k*n_head = 512.0`, and the line\n",
    " `self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)` will throw this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(fields['text'].vocab)\n",
    "d_model = 512\n",
    "n_layers = 6\n",
    "n_head = 8\n",
    "d_k = int(d_model / n_head)\n",
    "d_inner = 1024\n",
    "enc = Encoder(n_src_vocab=src_vocab_size, d_word_vec=d_model, n_layers=n_layers, n_head=n_head, \n",
    "              d_k=d_k, d_v=d_k, d_model=d_model, d_inner=d_inner, pad_idx=src_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequence shape:torch.Size([2, 49])\n"
     ]
    }
   ],
   "source": [
    "src_seq = small_batch.text.transpose(0,1)\n",
    "print('Source sequence shape:{}'.format(src_seq.shape))\n",
    "src_pad_mask = get_pad_mask(src_seq, src_pad_idx)\n",
    "src_subseq_mask = get_subsequent_mask(src_seq)\n",
    "src_mask = src_pad_mask & src_subseq_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,   461,  6476,    40,   780,     6,    24,   266,   305,  6943,\n",
       "            5,  9759,  3787,    15,     4,   506,     9,  4144,  1526,     5,\n",
       "            8,   343,   429,     9, 11562,  1396,     5,    35,     9,    61,\n",
       "         1382,   131,     5,   155,    58,    29,     4,   636,    97,   854,\n",
       "           22,   575,    63,     3,     1,     1,     1,     1,     1],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_seq[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   14,   11,    8,  698,    6,  135,    6, 1481,   17,   51,    4,\n",
       "         561, 3129,    5,   13,   20,   35,    9,  166,   19, 1233,   22,  180,\n",
       "        3876, 6334,   19,   89,   16, 1599,   62,   53, 1647,    5,   52,   25,\n",
       "          41,    4,   93,   15,   14,   35,    6,  199,   13,    8,  387,    5,\n",
       "           3], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_seq[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False,  ..., False, False, False],\n",
      "        [ True,  True, False,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(src_mask[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False,  ..., False, False, False],\n",
      "        [ True,  True, False,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ...,  True, False, False],\n",
      "        [ True,  True,  True,  ...,  True,  True, False],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(src_mask[1,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One has to explicitly place the model on the GPU or else, it will reside on the CPU\n",
    "* `RuntimeError: Input, output and indices must be on the current device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = enc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_op, enc_self_attn_list = enc(src_seq=src_seq, src_mask=src_mask, return_attns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_op.shape:torch.Size([2, 49, 512])\n",
      "enc_self_attn_list shapes:[torch.Size([2, 8, 49, 49]), torch.Size([2, 8, 49, 49]), torch.Size([2, 8, 49, 49]), torch.Size([2, 8, 49, 49]), torch.Size([2, 8, 49, 49]), torch.Size([2, 8, 49, 49])]\n"
     ]
    }
   ],
   "source": [
    "print('enc_op.shape:{}'.format(enc_op.shape))\n",
    "print('enc_self_attn_list shapes:{}'.format([item.shape for item in enc_self_attn_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.6745, 0.4366, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3794, 0.4647, 0.2670,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0346, 0.0122, 0.0348,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0323, 0.0115, 0.0353,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0198, 0.0138, 0.0248,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(enc_self_attn_list[0][0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3243, 0.3930, 0.3938,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0202, 0.0173, 0.0317,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0205, 0.0239, 0.0242,  ..., 0.0276, 0.0320, 0.0000],\n",
      "        [0.0262, 0.0180, 0.0191,  ..., 0.0230, 0.0230, 0.0161]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(enc_self_attn_list[0][1,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.4034, 0.7077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.2433, 0.3795, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0296, 0.0377,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0210, 0.0000, 0.0321,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0187, 0.0236, 0.0324,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(enc_self_attn_list[-1][0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.8213, 0.2898, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.4633, 0.2447, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0134, 0.0312, 0.0221,  ..., 0.0298, 0.0000, 0.0000],\n",
      "        [0.0141, 0.0180, 0.0308,  ..., 0.0000, 0.0335, 0.0000],\n",
      "        [0.0294, 0.0168, 0.0000,  ..., 0.0255, 0.0186, 0.0247]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(enc_self_attn_list[-1][1,0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier with learnable pooling \n",
    "\n",
    "* the below code performs $R^{2 \\times 65 \\times 512} \\rightarrow R^{2 \\times 1 \\times 512}$ assuming  $d\\_model \\in R^{512}$, $enc\\_output \\in R^{2 \\times 65 \\times 512}$, and the batch size is 2. The same is implemented in the below class `ClassificationHeadWithLearnablePooling` lines `10-14`.\n",
    "\n",
    "`temp_layer = nn.Linear(d_model, 1)`\n",
    "\n",
    "`temp_layer_op = temp_layer(enc_output)`\n",
    "\n",
    "`print(temp_layer_op.shape)`\n",
    "\n",
    "torch.Size([2, 65, 1])\n",
    "\n",
    "`temp_layer_op = temp_layer_op.transpose(-1, 1)`\n",
    "\n",
    "`temp_layer_op = F.softmax(temp_layer_op, dim=-1)` #softmax(g(XL)T) in R^{bx1xn}\n",
    "\n",
    "`print(temp_layer_op.shape)`\n",
    "\n",
    "torch.Size([2, 1, 65])\n",
    "\n",
    "`temp_z = torch.matmul(temp_layer_op, enc_output)` #[2,1,65]x[2,65,512], softmax(g(XL)T) x XL in R^{bx1xd} \n",
    "\n",
    "`print(temp_z.shape)`\n",
    "\n",
    "torch.Size([2, 1, 512])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHeadWithLearnablePooling(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, n_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.reduction_layer = nn.Linear(d_model, 1)\n",
    "        self.layer_norm = nn.LayerNorm(d_model) \n",
    "        self.linear_layer = nn.Linear(d_model, n_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ### Learnable pooling\n",
    "        reduction_layer_op = self.reduction_layer(x)\n",
    "        reduction_layer_op = reduction_layer_op.transpose(-1, 1)\n",
    "        reduction_layer_op = F.softmax(reduction_layer_op, dim=-1)\n",
    "        enc_output = torch.matmul(reduction_layer_op, x)\n",
    "        enc_output = enc_output.squeeze(1) #converts torch.Size([2, 1, 512]) -> torch.Size([2, 512])\n",
    "        ## end of learnable pooling\n",
    "        \n",
    "        ## layer norm and a fully connected layer\n",
    "        layer_normed_reduced = self.layer_norm(enc_output)\n",
    "        output = self.linear_layer(layer_normed_reduced)\n",
    "        #output = self.linear_layer(enc_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, pad_idx, n_classes, dropout=0.1, n_position=300, scale_emb=False,\n",
    "    return_attns=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.return_attns = return_attns\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            n_src_vocab=n_src_vocab, n_position=n_position,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            pad_idx=src_pad_idx, dropout=dropout, scale_emb=scale_emb)\n",
    "        \n",
    "        self.classifier_head = ClassificationHeadWithLearnablePooling(d_model=d_model, \n",
    "                                                                      n_classes=n_classes)\n",
    "    \n",
    "    def forward(self, src_seq: torch.Tensor) -> torch.Tensor:\n",
    "        src_mask = get_pad_mask(src_seq, self.src_pad_idx) & get_subsequent_mask(src_seq)\n",
    "        encoder_op, enc_self_attn = self.encoder(src_seq, src_mask,return_attns=self.return_attns)\n",
    "        classifier_op = self.classifier_head(encoder_op)\n",
    "        return classifier_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the complete sentiment analysis transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(fields['text'].vocab)\n",
    "d_model = 512\n",
    "n_layers = 6\n",
    "n_head = 8\n",
    "d_k = int(d_model / n_head)\n",
    "d_inner = 1024\n",
    "n_classes = 2\n",
    "sentiment_trf = SentimentTransformer(n_src_vocab=src_vocab_size, d_word_vec=d_model, n_layers=n_layers,\n",
    "                                    n_head=n_head, d_k=d_k, d_v=d_k, d_model=d_model, d_inner=d_inner,\n",
    "                                    pad_idx=src_pad_idx, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_trf = sentiment_trf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequence shape:torch.Size([2, 49])\n"
     ]
    }
   ],
   "source": [
    "src_seq = small_batch.text.transpose(0,1)\n",
    "print('Source sequence shape:{}'.format(src_seq.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_op = sentiment_trf(src_seq=src_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier_op.shape:torch.Size([2, 2])\n",
      "classifier_op:tensor([[-0.9916,  0.6943],\n",
      "        [-1.0393,  0.9186]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('classifier_op.shape:{}'.format(classifier_op.shape))\n",
    "print('classifier_op:{}'.format(classifier_op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37_env",
   "language": "python",
   "name": "python37_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
